{"cells":[{"cell_type":"code","source":["# For Google Colaboratory\n","!pip install pyspark py4j\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4vwJwBEhywq","executionInfo":{"status":"ok","timestamp":1756801512811,"user_tz":-480,"elapsed":26737,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}},"outputId":"507881fd-d82e-4f46-f056-d385db336483"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.7)\n"]}]},{"cell_type":"code","source":["from pyspark.sql.types import *\n","from pyspark.sql.functions import *"],"metadata":{"id":"MIxVNyhJhzyg","executionInfo":{"status":"ok","timestamp":1756801516261,"user_tz":-480,"elapsed":40,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# For Google Colaboratory\n","import sys, os\n","if 'google.colab' in sys.modules:\n","    # mount google drive\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    path_to_file = '/content/gdrive/MyDrive/Big_Data/Practicals' # Please adjust the path accordingly\n","    os.chdir(path_to_file)\n","    !pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGrgJg4wh2Ys","executionInfo":{"status":"ok","timestamp":1756801536109,"user_tz":-480,"elapsed":18846,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}},"outputId":"da569026-0d5c-42e9-eb36-5f94297ba96e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Big_Data/Practicals\n"]}]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"f6ce3d7e-badc-470c-ab83-438a82145b65","showTitle":false,"title":""},"id":"X6IKBRd6hwKt"},"source":["# Practical 2d: Hyperparameter Tuning\n","\n","Let's perform hyperparameter tuning on a random forest to find the best hyperparameters!"]},{"cell_type":"code","execution_count":4,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c8938e62-065f-4450-8d4a-8d70e2772bdb","showTitle":false,"title":""},"id":"0sLQXhH5hwKu","executionInfo":{"status":"ok","timestamp":1756801554456,"user_tz":-480,"elapsed":9940,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["from pyspark.ml.feature import StringIndexer, VectorAssembler\n","\n","filePath = \"file:/content/gdrive/MyDrive/Big_Data/Practicals/sf-airbnb-clean.parquet\"\n","airbnbDF = spark.read.parquet(filePath)\n","(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n","\n","categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n","indexOutputCols = [x + \"Index\" for x in categoricalCols]\n","\n","stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n","\n","numericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n","assemblerInputs = indexOutputCols + numericCols\n","vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"f250dea2-8bf9-4b0c-b057-ab20fcd8b7cb","showTitle":false,"title":""},"id":"Mvbth3PNhwKv"},"source":["## Random Forest"]},{"cell_type":"code","execution_count":5,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c0e3e436-af5b-427a-8032-b40d779b58df","showTitle":false,"title":""},"id":"KTc6K-jWhwKw","executionInfo":{"status":"ok","timestamp":1756801559782,"user_tz":-480,"elapsed":58,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.ml import Pipeline\n","\n","rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed=42)\n","pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"167b607d-8b34-4896-be43-7f17329ebd7f","showTitle":false,"title":""},"id":"A4RLdVRbhwKw"},"source":["## Grid Search\n","\n","There are a lot of hyperparameters we could tune, and it would take a long time to manually configure.\n","\n","Let's use Spark's `ParamGridBuilder` to find the optimal hyperparameters in a more systematic approach [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.tuning.ParamGridBuilder).\n","\n","Let's define a grid of hyperparameters to test:\n","  - maxDepth: max depth of the decision tree (Use the values `2, 4, 6`)\n","  - numTrees: number of decision trees (Use the values `10, 100`)"]},{"cell_type":"code","execution_count":6,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"626d6825-d0eb-4f85-b073-efe7e1c8a408","showTitle":false,"title":""},"id":"y4V_9ZEIhwKw","executionInfo":{"status":"ok","timestamp":1756801562648,"user_tz":-480,"elapsed":2,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["from pyspark.ml.tuning import ParamGridBuilder\n","\n","paramGrid = (ParamGridBuilder()\n","            .addGrid(rf.maxDepth, [2, 4, 6])\n","            .addGrid(rf.numTrees, [10, 100])\n","            .build())"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"5b2bdb17-6dd5-4345-9098-4aaf9c3f9b81","showTitle":false,"title":""},"id":"SYyq2Oh7hwKw"},"source":["## Cross Validation\n","\n","We are also going to use 3-fold cross validation to identify the optimal maxDepth.\n","\n","![crossValidation](https://files.training.databricks.com/images/301/CrossValidation.png)\n","\n","With 3-fold cross-validation, we train on 2/3 of the data, and evaluate with the remaining (held-out) 1/3. We repeat this process 3 times, so each fold gets the chance to act as the validation set. We then average the results of the three rounds."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c4b33850-856f-4c8e-a232-fc7abef888ac","showTitle":false,"title":""},"id":"9EZCi2B5hwKx"},"source":["We pass in the `estimator` (pipeline), `evaluator`, and `estimatorParamMaps` to `CrossValidator` so that it knows:\n","- Which model to use\n","- How to evaluate the model\n","- What hyperparameters to set for the model\n","\n","We can also set the number of folds we want to split our data into (3), as well as setting a seed so we all have the same split in the data [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.tuning.CrossValidator)."]},{"cell_type":"code","execution_count":7,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1d6d6f9c-ad29-42d4-9c81-403a5e093f9d","showTitle":false,"title":""},"id":"ZBg1gKn8hwKx","executionInfo":{"status":"ok","timestamp":1756801566944,"user_tz":-480,"elapsed":10,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.tuning import CrossValidator\n","\n","evaluator = RegressionEvaluator(labelCol=\"price\",\n","                                predictionCol=\"prediction\",\n","                                metricName=\"rmse\")\n","\n","cv = CrossValidator(estimator=pipeline,\n","                    evaluator=evaluator,\n","                    estimatorParamMaps=paramGrid,\n","                    numFolds=3,\n","                    seed=42)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b6a990d4-a61d-45df-8797-5b9510ca6042","showTitle":false,"title":""},"id":"vi9wgS2ZhwKx"},"source":["**Question**: How many models are we training right now?"]},{"cell_type":"code","execution_count":8,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1eee4a27-ce5c-48f4-a024-4498ef8c8eca","showTitle":false,"title":""},"id":"-TvdWXFqhwKx","executionInfo":{"status":"ok","timestamp":1756801668510,"user_tz":-480,"elapsed":98570,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["cvModel = cv.fit(trainDF)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"fad5b5b4-1275-4bb6-a630-4eee1c477f78","showTitle":false,"title":""},"id":"ukRmFSophwKx"},"source":["## Parallelism Parameter\n","\n","Hmmm... that took a long time to run. That's because the models were being trained sequentially rather than in parallel!\n","\n","Spark 2.3 introduced a [parallelism](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator.parallelism) parameter. From the docs: `the number of threads to use when running parallel algorithms (>= 1)`.\n","\n","Let's set this value to 4 and see if we can train any faster."]},{"cell_type":"code","execution_count":9,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"737bc528-bc89-4115-a347-1d00cdd93233","showTitle":false,"title":""},"id":"PcFtNG_uhwKx","executionInfo":{"status":"ok","timestamp":1756801889314,"user_tz":-480,"elapsed":52349,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["cvModel = cv.setParallelism(4).fit(trainDF)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"030ddc15-efbc-4014-a3dd-483ad3644519","showTitle":false,"title":""},"id":"s9NXxkgUhwKy"},"source":["**Question**: Hmmm... that still took a long time to run. Should we put the pipeline in the cross validator, or the cross validator in the pipeline?\n","\n","It depends if there are estimators or transformers in the pipeline. If you have things like StringIndexer (an estimator) in the pipeline, then you have to refit it every time if you put the entire pipeline in the cross validator."]},{"cell_type":"code","execution_count":10,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3a92dfc1-e99d-4733-bc06-345c42e42877","showTitle":false,"title":""},"id":"kCm-X-DThwKy","executionInfo":{"status":"ok","timestamp":1756801939173,"user_tz":-480,"elapsed":45088,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["cv = CrossValidator(estimator=rf,\n","                    evaluator=evaluator,\n","                    estimatorParamMaps=paramGrid,\n","                    numFolds=3,\n","                    parallelism=4,\n","                    seed=42)\n","\n","pipeline = Pipeline(stages=[stringIndexer, vecAssembler, cv])\n","\n","pipelineModel = pipeline.fit(trainDF)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"a3f5eaee-a8e2-4ed9-83c4-174c23d3f584","showTitle":false,"title":""},"id":"knJt3cREhwKy"},"source":["Let's take a look at the model with the best hyperparameter configuration"]},{"cell_type":"code","execution_count":11,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"a2fb0675-3b53-4b5a-a6f9-0a69429f6c4a","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/"},"id":"xlZOXyQZhwKy","executionInfo":{"status":"ok","timestamp":1756801941019,"user_tz":-480,"elapsed":26,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}},"outputId":"41921ebb-7e9d-4795-944b-c16589b36c15"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[({Param(parent='RandomForestRegressor_5a832a1b2bf9', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n","   Param(parent='RandomForestRegressor_5a832a1b2bf9', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n","  np.float64(291.1822640924783)),\n"," ({Param(parent='RandomForestRegressor_5a832a1b2bf9', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n","   Param(parent='RandomForestRegressor_5a832a1b2bf9', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n","  np.float64(286.7714750274078)),\n"," ({Param(parent='RandomForestRegressor_5a832a1b2bf9', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n","   Param(parent='RandomForestRegressor_5a832a1b2bf9', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n","  np.float64(287.6963245160818)),\n"," ({Param(parent='RandomForestRegressor_5a832a1b2bf9', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n","   Param(parent='RandomForestRegressor_5a832a1b2bf9', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n","  np.float64(279.9927057236079)),\n"," ({Param(parent='RandomForestRegressor_5a832a1b2bf9', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n","   Param(parent='RandomForestRegressor_5a832a1b2bf9', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n","  np.float64(294.34810870889305)),\n"," ({Param(parent='RandomForestRegressor_5a832a1b2bf9', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n","   Param(parent='RandomForestRegressor_5a832a1b2bf9', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n","  np.float64(275.39862704729984))]"]},"metadata":{},"execution_count":11}],"source":["list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"f0522075-2720-4d2e-be89-cb95d0049110","showTitle":false,"title":""},"id":"WYg6KAe2hwKy"},"source":["\n","Let's see how it does on the test dataset."]},{"cell_type":"code","execution_count":12,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3a0012b6-9c15-46ce-97df-1f526f089610","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/"},"id":"L--7xHdqhwKy","executionInfo":{"status":"ok","timestamp":1756801944332,"user_tz":-480,"elapsed":1197,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}},"outputId":"c84f6026-2c57-421a-f00e-373aaba3b5fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["RMSE is 211.70370310223277\n","R2 is 0.2265254865671944\n"]}],"source":["predDF = pipelineModel.transform(testDF)\n","\n","regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n","\n","rmse = regressionEvaluator.evaluate(predDF)\n","r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n","print(f\"RMSE is {rmse}\")\n","print(f\"R2 is {r2}\")\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Practical_2d","widgets":{}},"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}