{"cells":[{"cell_type":"code","source":["# For Google Colaboratory\n","!pip install pyspark py4j\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-eS1GBzpgyQD","executionInfo":{"status":"ok","timestamp":1756801250490,"user_tz":-480,"elapsed":15904,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}},"outputId":"bb9bdd93-592d-447e-aabd-f88b472a4314"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.7)\n"]}]},{"cell_type":"code","source":["from pyspark.sql.types import *\n","from pyspark.sql.functions import *"],"metadata":{"id":"XBWpKcMeg2js","executionInfo":{"status":"ok","timestamp":1756801271502,"user_tz":-480,"elapsed":38,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# For Google Colaboratory\n","import sys, os\n","if 'google.colab' in sys.modules:\n","    # mount google drive\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    path_to_file = '/content/gdrive/MyDrive/Big_Data/Practicals' # Please adjust the path accordingly\n","    os.chdir(path_to_file)\n","    !pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x7m9VuO_g5ON","executionInfo":{"status":"ok","timestamp":1756801297825,"user_tz":-480,"elapsed":16775,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}},"outputId":"b888b52f-909a-48f9-c88d-45a513d7d6c9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Big_Data/Practicals\n"]}]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"27147a2f-5b23-4c92-851b-24c7b43ccf13","showTitle":false,"title":""},"id":"U2CYUOyLgxh7"},"source":["# Practical 2c: One-Hot Encoding\n","\n","In this notebook we will be adding additional features to our model, as well as discuss how to handle categorical features."]},{"cell_type":"code","execution_count":4,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3eecccb8-1693-4cc5-bfc0-b9d419721fde","showTitle":false,"title":""},"id":"6k1NITdRgxh8","executionInfo":{"status":"ok","timestamp":1756801317227,"user_tz":-480,"elapsed":6146,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["filePath = \"file:/content/gdrive/MyDrive/Big_Data/Practicals/sf-airbnb-clean.parquet\"\n","airbnbDF = spark.read.parquet(filePath)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1e6e9350-5158-4b01-b8dc-faddd67ff0c7","showTitle":false,"title":""},"id":"tcFlOOnEgxh9"},"source":["## Train/Test Split\n","\n","Let's use the same 80/20 split with the same seed as the previous notebook so we can compare our results apples to apples (unless you changed the cluster config!)"]},{"cell_type":"code","execution_count":5,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"131fd6c7-2e84-4810-9194-603261ebfa79","showTitle":false,"title":""},"id":"F-vWs1EBgxh9","executionInfo":{"status":"ok","timestamp":1756801334178,"user_tz":-480,"elapsed":81,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8b4ecb6f-8255-40cb-9c42-96db2b9210b0","showTitle":false,"title":""},"id":"vnZRU-p0gxh-"},"source":["## Option 1: StringIndexer, OneHotEncoder, and VectorAssembler\n","\n","Here, we are going to One Hot Encode (OHE) our categorical variables. The first approach we are going to use will combine StringIndexer, OneHotEncoder, and VectorAssembler.\n","\n","First we need to use `StringIndexer` to map a string column of labels to an ML column of label indices [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.StringIndexer).\n","\n","Then, we can apply the `OneHotEncoder` to the output of the StringIndexer [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.OneHotEncoder)."]},{"cell_type":"code","execution_count":6,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b06b81fe-2591-4ff9-a132-be2ed6e64973","showTitle":false,"title":""},"id":"aZlfdv8Ugxh-","executionInfo":{"status":"ok","timestamp":1756801336594,"user_tz":-480,"elapsed":312,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["from pyspark.ml.feature import OneHotEncoder, StringIndexer\n","\n","categoricalCols = [field for (field, dataType) in trainDF.dtypes\n","                   if dataType == \"string\"]\n","indexOutputCols = [x + \"Index\" for x in categoricalCols]\n","oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n","\n","stringIndexer = StringIndexer(inputCols=categoricalCols,\n","                              outputCols=indexOutputCols,\n","                              handleInvalid=\"skip\")\n","oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n","                           outputCols=oheOutputCols)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"70940f32-f42d-4c8b-a638-1ab881aeef12","showTitle":false,"title":""},"id":"bXrq3BQBgxh-"},"source":["Now we can combine our OHE categorical features with our numeric features."]},{"cell_type":"code","execution_count":7,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"696e4cc9-0242-4653-ab20-d9afb24747c6","showTitle":false,"title":""},"id":"w9s0r_Togxh-","executionInfo":{"status":"ok","timestamp":1756801338698,"user_tz":-480,"elapsed":19,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["from pyspark.ml.feature import VectorAssembler\n","\n","numericCols = [field for (field, dataType) in trainDF.dtypes\n","               if ((dataType == \"double\") & (field != \"price\"))]\n","assemblerInputs = oheOutputCols + numericCols\n","vecAssembler = VectorAssembler(inputCols=assemblerInputs,\n","                               outputCol=\"features\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d646025a-4b12-40fd-bc04-54cdfa979c82","showTitle":false,"title":""},"id":"sUYED9tzgxh_"},"source":["## Option 2: RFormula\n","Instead of manually specifying which columns are categorical to the StringIndexer and OneHotEncoder, RFormula can do that automatically for you [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RFormula)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.feature.RFormula).\n","\n","With RFormula, if you have any columns of type String, it treats it as a categorical feature and string indexes & one hot encodes it for us. Otherwise, it leaves as it is. Then it combines all of one-hot encoded features and numeric features into a single vector, called `features`."]},{"cell_type":"code","execution_count":8,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"1063ff1d-118e-4da3-bbbb-7b034edca9a8","showTitle":false,"title":""},"id":"sScw4v7Zgxh_","executionInfo":{"status":"ok","timestamp":1756801341214,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["from pyspark.ml.feature import RFormula\n","\n","rFormula = RFormula(formula=\"price ~ .\", featuresCol=\"features\", labelCol=\"price\", handleInvalid=\"skip\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4b79f3ee-ef79-4bd9-9895-f2b60ff02439","showTitle":false,"title":""},"id":"DHul5h-Wgxh_"},"source":["## Linear Regression\n","\n","Now that we have all of our features, let's build a linear regression model."]},{"cell_type":"code","execution_count":9,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"2a2a9d4a-0261-4090-9610-699412411cee","showTitle":false,"title":""},"id":"u9UtOH0zgxh_","executionInfo":{"status":"ok","timestamp":1756801343921,"user_tz":-480,"elapsed":61,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["from pyspark.ml.regression import LinearRegression\n","\n","lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ceebe97f-09dc-48e9-95c8-a6461c879185","showTitle":false,"title":""},"id":"4GmohJbMgxh_"},"source":["## Pipeline\n","\n","Let's put all these stages in a Pipeline. A `Pipeline` is a way of organizing all of our transformers and estimators [Python](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline)/[Scala](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.Pipeline).\n","\n","Verify you get the same results with Option 1 (StringIndexer, OneHotEncoderEstimator, and VectorAssembler) and Option 2 (RFormula)"]},{"cell_type":"code","execution_count":10,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"749a031d-e8b5-4310-9d81-a02ddc41b903","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/"},"id":"htiR1WOggxiA","executionInfo":{"status":"ok","timestamp":1756801360996,"user_tz":-480,"elapsed":15338,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}},"outputId":"c356ee24-fd80-4439-b883-658ddc9765e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+-----+------------------+\n","|            features|price|        prediction|\n","+--------------------+-----+------------------+\n","|(98,[0,3,6,22,43,...| 85.0|  55.7117331960344|\n","|(98,[0,3,6,22,43,...| 45.0| 23.11121524248665|\n","|(98,[0,3,6,22,43,...| 70.0|27.644339230358128|\n","|(98,[0,3,6,12,42,...|128.0|-92.40433634418105|\n","|(98,[0,3,6,12,43,...|159.0| 94.84679937388546|\n","+--------------------+-----+------------------+\n","only showing top 5 rows\n","\n"]}],"source":["# Option 1: StringIndexer + OHE + VectorAssembler\n","from pyspark.ml import Pipeline\n","\n","stages = [stringIndexer, oheEncoder, vecAssembler, lr]\n","pipeline = Pipeline(stages=stages)\n","\n","pipelineModel = pipeline.fit(trainDF)\n","predDF = pipelineModel.transform(testDF)\n","predDF.select(\"features\", \"price\", \"prediction\").show(5)"]},{"cell_type":"code","execution_count":11,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"a219887f-b888-407a-ab0d-af555f5f4ce8","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/"},"id":"u0LbUe_hgxiA","executionInfo":{"status":"ok","timestamp":1756801371167,"user_tz":-480,"elapsed":8108,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}},"outputId":"bb1f144b-64e6-40db-c896-0388fdc04375"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+-----+------------------+\n","|            features|price|        prediction|\n","+--------------------+-----+------------------+\n","|(98,[0,3,6,7,23,4...| 85.0| 55.30094763354373|\n","|(98,[0,3,6,7,23,4...| 45.0| 22.70940291742818|\n","|(98,[0,3,6,7,23,4...| 70.0|27.182906571761578|\n","|(98,[0,3,6,7,13,4...|128.0|-91.90969569190747|\n","|(98,[0,3,6,7,13,4...|159.0| 94.54162775821169|\n","+--------------------+-----+------------------+\n","only showing top 5 rows\n","\n"]}],"source":["# Option 2: RFormula\n","from pyspark.ml import Pipeline\n","\n","pipeline = Pipeline(stages = [rFormula, lr])\n","\n","pipelineModel = pipeline.fit(trainDF)\n","predDF = pipelineModel.transform(testDF)\n","predDF.select(\"features\", \"price\", \"prediction\").show(5)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0aeea7e7-96bc-41d9-a9dd-6647034b939b","showTitle":false,"title":""},"id":"mtde9PEBgxiA"},"source":["## Evaluate Model: RMSE"]},{"cell_type":"code","execution_count":13,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"c71a0e37-425d-4391-bb1b-627af45473fd","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/"},"id":"RMJIszmCgxiA","executionInfo":{"status":"ok","timestamp":1756801385502,"user_tz":-480,"elapsed":433,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}},"outputId":"6bdc1110-77fd-404c-abd4-f5a0a37d4d9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["RMSE is 220.63960308537358\n"]}],"source":["from pyspark.ml.evaluation import RegressionEvaluator\n","\n","regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n","\n","rmse = regressionEvaluator.evaluate(predDF)\n","print(f\"RMSE is {rmse}\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4d0a0acc-4b0d-4bdf-9798-cacbc88e9286","showTitle":false,"title":""},"id":"8iHtUqyRgxiA"},"source":["## R2\n","\n","![](https://files.training.databricks.com/images/r2d2.jpg) How is our R2 doing?"]},{"cell_type":"code","execution_count":14,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"5c7e070e-8e2a-4852-96f8-8ad460a5eed7","showTitle":false,"title":""},"colab":{"base_uri":"https://localhost:8080/"},"id":"-lEK1ooBgxiA","executionInfo":{"status":"ok","timestamp":1756801406487,"user_tz":-480,"elapsed":394,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}},"outputId":"9c35884c-4fd4-4c02-e016-497d91807b40"},"outputs":[{"output_type":"stream","name":"stdout","text":["R2 is 0.15985154393435386\n"]}],"source":["r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n","print(f\"R2 is {r2}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"2624e1f6-f169-437b-a119-8bbc4fc676ac","showTitle":false,"title":""},"id":"Uib3Uu2fgxiB","executionInfo":{"status":"ok","timestamp":1756801443409,"user_tz":-480,"elapsed":7150,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["pipelinePath = \"file:/content/gdrive/MyDrive/Big_Data/Practicals/lr-pipeline-model\"\n","pipelineModel.write().overwrite().save(pipelinePath)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6b42ed2f-4b6e-4d0f-836b-e66d04fcb81a","showTitle":false,"title":""},"id":"J_GMcNSigxiB"},"source":["## Loading models\n","\n","When you load in models, you need to know the type of model you are loading back in (was it a linear regression or logistic regression model?).\n","\n","For this reason, we recommend you always put your transformers/estimators into a Pipeline, so you can always load the generic PipelineModel back in."]},{"cell_type":"code","execution_count":16,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b48e2abd-21ea-44cf-b4e3-56a5ee38d7b4","showTitle":false,"title":""},"id":"5HpoDDx6gxiB","executionInfo":{"status":"ok","timestamp":1756801455731,"user_tz":-480,"elapsed":6392,"user":{"displayName":"Xin AI","userId":"08487103376102314083"}}},"outputs":[],"source":["from pyspark.ml import PipelineModel\n","\n","savedPipelineModel = PipelineModel.load(pipelinePath)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"dd7aeb0a-fc15-48ea-bc9a-4bb9831a2515","showTitle":false,"title":""},"id":"22FPolrAgxiB"},"source":["## Distributed Setting\n","\n","If you are interested in learning how linear regression is implemented in the distributed setting and bottlenecks, check out these lecture slides:\n","* [distributed-linear-regression-1](https://files.training.databricks.com/static/docs/distributed-linear-regression-1.pdf)\n","* [distributed-linear-regression-2](https://files.training.databricks.com/static/docs/distributed-linear-regression-2.pdf)\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Practical_2c","widgets":{}},"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}